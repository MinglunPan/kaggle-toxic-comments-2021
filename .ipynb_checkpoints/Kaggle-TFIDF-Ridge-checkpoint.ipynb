{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "767f5232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75a008f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e28ff86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Embedding' from '/home/adrian/Projects/Competition/kaggle-toxic-comments-2021/Embedding.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Config import *\n",
    "import TextDataFrame as tdf\n",
    "import utils\n",
    "import Embedding as eb\n",
    "\n",
    "import imp\n",
    "imp.reload(tdf)\n",
    "imp.reload(utils)\n",
    "imp.reload(eb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9875356",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ccee24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processor = tdf.DatasetProcessor(DEFAULT_CLEAN_PROCEDURE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f38709",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7485d572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f5ef286bf546c8be39de74fa65ddb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights_target_dict = {'obscene': 1, 'toxic': 1, 'threat': 1,  'insult': 1, 'severe_toxic': 2, 'identity_hate': 1}\n",
    "text_col = 'comment_text'\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(DATA_PATH,\"toxic-comment-classification-challenge/train.csv\"))\n",
    "df_train['y'] = df_processor.set_target(df_train, average_weights_dict = weights_target_dict)\n",
    "df_train[text_col] = df_processor.clean_text(df_train[text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cd04c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = eb.Embedding()\n",
    "tokenizer.fit(df_train[text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35f1e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_new = utils.sample_binary(df_train, 'y')\n",
    "y = df_train_new['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85151dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TFIDF = tokenizer.transform('TFIDF', df_train_new[text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0e4e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_BERT = tokenizer.transform('BERT', df_train_new[text_col], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908d36cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model_ridge_BERT = Ridge(alpha=0.5)\n",
    "model_ridge_TFIDF = Ridge(alpha=0.5)\n",
    "\n",
    "model_ridge_TFIDF.fit(X_TFIDF, y)\n",
    "model_ridge_BERT.fit(X_BERT, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef5a9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text_series, text_cleaner_func, tokenizer_transform_func, predict_func,\n",
    "           ):\n",
    "    cleaned_text = text_cleaner_func(text_series)\n",
    "    cleaned_text = tokenizer_transform_func(cleaned_text)\n",
    "    probability =  predict_func(cleaned_text)\n",
    "    return probability\n",
    "def evaluate(text_cleaner_func, tokenizer_transform_func, predict_func):\n",
    "\n",
    "    df_val = pd.read_csv(os.path.join(DATA_PATH,\"toxic-severity-rating/validation_data.csv\"))\n",
    "\n",
    "    p1 = predict(df_val['less_toxic'], \n",
    "                 text_cleaner_func, \n",
    "                 tokenizer_transform_func,\n",
    "                 predict_func\n",
    "                )\n",
    "\n",
    "    p2 = predict(df_val['more_toxic'], \n",
    "                 text_cleaner_func, \n",
    "                 tokenizer_transform_func,\n",
    "                 predict_func\n",
    "                )\n",
    "\n",
    "    return {\"% Correct\":(p1<p2).mean(), \"% Equal\":(p1==p2).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa0db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.read_csv(os.path.join(DATA_PATH,\"toxic-severity-rating/validation_data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a36fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_series = df_val['less_toxic']\n",
    "cleaned_text =  df_processor.clean_text(text_series)\n",
    "cleaned_text = tokenizer.transform('TFIDF', cleaned_text)\n",
    "p1 = model_ridge_TFIDF.predict(cleaned_text)\n",
    "\n",
    "text_series = df_val['more_toxic']\n",
    "cleaned_text =  df_processor.clean_text(text_series)\n",
    "cleaned_text = tokenizer.transform('TFIDF', cleaned_text)\n",
    "p2 = model_ridge_TFIDF.predict(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c927b135",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"% Correct\":(p1<p2).mean(), \"% Equal\":(p1==p2).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f33fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_series = df_val['less_toxic']\n",
    "cleaned_text =  df_processor.clean_text(text_series)\n",
    "cleaned_text = tokenizer.transform('BERT', cleaned_text, 100)\n",
    "p1_BERT = model_ridge_BERT.predict(cleaned_text)\n",
    "np.save('less_toxic.npy', cleaned_text)\n",
    "\n",
    "text_series = df_val['more_toxic']\n",
    "cleaned_text =  df_processor.clean_text(text_series)\n",
    "cleaned_text = tokenizer.transform('BERT', cleaned_text, 100)\n",
    "p2_BERT = model_ridge_BERT.predict(cleaned_text)\n",
    "np.save('more_toxic.npy', cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c1bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"% Correct\":(p1_BERT<p2_BERT).mean(), \"% Equal\":(p1_BERT==p2_BERT).mean()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
